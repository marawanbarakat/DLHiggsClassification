{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:GPU:0']\n",
      "50-nodes-1-dense-1591114557\n",
      "(54491, 30)\n",
      "Train on 54491 samples\n",
      "Epoch 1/20\n",
      "54491/54491 [==============================] - 8s 147us/sample - loss: 0.4386 - accuracy: 0.7936\n",
      "Epoch 2/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3813 - accuracy: 0.8331\n",
      "Epoch 3/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3747 - accuracy: 0.8372\n",
      "Epoch 4/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3709 - accuracy: 0.8378\n",
      "Epoch 5/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3677 - accuracy: 0.8382\n",
      "Epoch 6/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3650 - accuracy: 0.8403\n",
      "Epoch 7/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3627 - accuracy: 0.8420\n",
      "Epoch 8/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3608 - accuracy: 0.8427\n",
      "Epoch 9/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3593 - accuracy: 0.8424\n",
      "Epoch 10/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3577 - accuracy: 0.8434\n",
      "Epoch 11/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3561 - accuracy: 0.8446\n",
      "Epoch 12/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3551 - accuracy: 0.8440\n",
      "Epoch 13/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3537 - accuracy: 0.8455\n",
      "Epoch 14/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3530 - accuracy: 0.8466\n",
      "Epoch 15/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3522 - accuracy: 0.8458\n",
      "Epoch 16/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3510 - accuracy: 0.8467\n",
      "Epoch 17/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3504 - accuracy: 0.8465\n",
      "Epoch 18/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3497 - accuracy: 0.8469\n",
      "Epoch 19/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3488 - accuracy: 0.8460\n",
      "Epoch 20/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3485 - accuracy: 0.8469\n",
      "100-nodes-1-dense-1591114713\n",
      "(54491, 30)\n",
      "Train on 54491 samples\n",
      "Epoch 1/20\n",
      "54491/54491 [==============================] - 8s 146us/sample - loss: 0.4278 - accuracy: 0.8040\n",
      "Epoch 2/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3790 - accuracy: 0.8343\n",
      "Epoch 3/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3728 - accuracy: 0.8362\n",
      "Epoch 4/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3681 - accuracy: 0.8389\n",
      "Epoch 5/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3650 - accuracy: 0.8397\n",
      "Epoch 6/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3618 - accuracy: 0.8402\n",
      "Epoch 7/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3594 - accuracy: 0.8420\n",
      "Epoch 8/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3576 - accuracy: 0.8428\n",
      "Epoch 9/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3551 - accuracy: 0.8436\n",
      "Epoch 10/20\n",
      "54491/54491 [==============================] - 8s 145us/sample - loss: 0.3533 - accuracy: 0.8440\n",
      "Epoch 11/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3514 - accuracy: 0.8462\n",
      "Epoch 12/20\n",
      "54491/54491 [==============================] - 8s 145us/sample - loss: 0.3494 - accuracy: 0.8467\n",
      "Epoch 13/20\n",
      "54491/54491 [==============================] - 8s 145us/sample - loss: 0.3477 - accuracy: 0.8477\n",
      "Epoch 14/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3463 - accuracy: 0.8478\n",
      "Epoch 15/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3452 - accuracy: 0.8473\n",
      "Epoch 16/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3431 - accuracy: 0.8492\n",
      "Epoch 17/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3422 - accuracy: 0.8489\n",
      "Epoch 18/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3409 - accuracy: 0.8502\n",
      "Epoch 19/20\n",
      "54491/54491 [==============================] - 8s 145us/sample - loss: 0.3397 - accuracy: 0.8504\n",
      "Epoch 20/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3388 - accuracy: 0.8515\n",
      "800-nodes-1-dense-1591114871\n",
      "(54491, 30)\n",
      "Train on 54491 samples\n",
      "Epoch 1/20\n",
      "54491/54491 [==============================] - 8s 146us/sample - loss: 0.4020 - accuracy: 0.8199\n",
      "Epoch 2/20\n",
      "54491/54491 [==============================] - 8s 142us/sample - loss: 0.3767 - accuracy: 0.8343\n",
      "Epoch 3/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3697 - accuracy: 0.8371\n",
      "Epoch 4/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3636 - accuracy: 0.8405\n",
      "Epoch 5/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3577 - accuracy: 0.8435\n",
      "Epoch 6/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3548 - accuracy: 0.8429\n",
      "Epoch 7/20\n",
      "54491/54491 [==============================] - 8s 145us/sample - loss: 0.3507 - accuracy: 0.8453\n",
      "Epoch 8/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3481 - accuracy: 0.8459\n",
      "Epoch 9/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3446 - accuracy: 0.8485\n",
      "Epoch 10/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3429 - accuracy: 0.8501\n",
      "Epoch 11/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3399 - accuracy: 0.8494\n",
      "Epoch 12/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3381 - accuracy: 0.8513\n",
      "Epoch 13/20\n",
      "54491/54491 [==============================] - 8s 145us/sample - loss: 0.3356 - accuracy: 0.8525\n",
      "Epoch 14/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3331 - accuracy: 0.8545\n",
      "Epoch 15/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3321 - accuracy: 0.8533\n",
      "Epoch 16/20\n",
      "54491/54491 [==============================] - 8s 145us/sample - loss: 0.3296 - accuracy: 0.8550\n",
      "Epoch 17/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3271 - accuracy: 0.8558\n",
      "Epoch 18/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3265 - accuracy: 0.8566\n",
      "Epoch 19/20\n",
      "54491/54491 [==============================] - 8s 143us/sample - loss: 0.3233 - accuracy: 0.8578\n",
      "Epoch 20/20\n",
      "54491/54491 [==============================] - 8s 144us/sample - loss: 0.3218 - accuracy: 0.8572\n",
      "50-nodes-5-dense-1591115028\n",
      "(54491, 30)\n",
      "Train on 54491 samples\n",
      "Epoch 1/20\n",
      "54491/54491 [==============================] - 9s 174us/sample - loss: 0.4066 - accuracy: 0.8131\n",
      "Epoch 2/20\n",
      "54491/54491 [==============================] - 9s 169us/sample - loss: 0.3661 - accuracy: 0.8391\n",
      "Epoch 3/20\n",
      "54491/54491 [==============================] - 9s 170us/sample - loss: 0.3576 - accuracy: 0.8427\n",
      "Epoch 4/20\n",
      "54491/54491 [==============================] - 9s 170us/sample - loss: 0.3523 - accuracy: 0.8443\n",
      "Epoch 5/20\n",
      "54491/54491 [==============================] - 9s 170us/sample - loss: 0.3489 - accuracy: 0.8462\n",
      "Epoch 6/20\n",
      "54491/54491 [==============================] - 9s 168us/sample - loss: 0.3455 - accuracy: 0.8481\n",
      "Epoch 7/20\n",
      "54491/54491 [==============================] - 9s 168us/sample - loss: 0.3420 - accuracy: 0.8496\n",
      "Epoch 8/20\n",
      "54491/54491 [==============================] - 9s 169us/sample - loss: 0.3396 - accuracy: 0.8505\n",
      "Epoch 9/20\n",
      "54491/54491 [==============================] - 9s 170us/sample - loss: 0.3374 - accuracy: 0.8513\n",
      "Epoch 10/20\n",
      "14240/54491 [======>.......................] - ETA: 6s - loss: 0.3281 - accuracy: 0.8558"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, optimizers, regularizers\n",
    "from tensorflow.keras.layers import Flatten , Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "Name=\"TensorBoard-{}\".format(int(time.time()))\n",
    "tensorboard= TensorBoard(log_dir='logs/{}'.format(Name ))\n",
    "#tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "\n",
    "def _get_available_gpus():\n",
    "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
    "    # Source of this function: https://github.com/keras-team/keras/issues/13684\n",
    "    # Returns\n",
    "        A list of available GPU devices.\n",
    "    \"\"\"\n",
    "    #global _LOCAL_DEVICES\n",
    "    if tfback._LOCAL_DEVICES is None:\n",
    "        devices = tf.config.list_logical_devices()\n",
    "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "tf.config.set_visible_devices(gpus[1], 'GPU')\n",
    "\n",
    "tfback._get_available_gpus = _get_available_gpus\n",
    "print(_get_available_gpus())\n",
    "\n",
    "        \n",
    "df =pd.read_csv('Training.csv')\n",
    "\n",
    "df = df.replace(-999, np.nan)\n",
    "df = df.dropna()\n",
    "df.shape\n",
    "df_droped = df.drop(['EventId', 'Weight'], axis=1)\n",
    "df_droped['Label'] = df_droped['Label'].apply({'s':1, 'b':0}.get)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_droped.drop(['Label'], axis=1), df_droped['Label'], test_size=0.2)\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dense_layers=[1,5,10]\n",
    "layer_sizes=[50,100,800]\n",
    "#conv_layers=\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        Name=\"{}-nodes-{}-dense-{}\".format(layer_size,dense_layer,int(time.time()))\n",
    "        print(Name)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "\n",
    "        for l in range(dense_layer):\n",
    "            \n",
    "            model.add(Dense(layer_size))\n",
    "            model.add(Activation('relu')) \n",
    "\n",
    "    \n",
    "\n",
    "        # Add an output layer \n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "        print(X_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "        model.fit(X_train,y_train, epochs=20 ,batch_size=32,callbacks=[tensorboard])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
